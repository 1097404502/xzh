# 写入空文件
```bash
dd if=/dev/zero of=/root/kk/test_zero bs=2G count=1

2147479552 bytes (2.1 GB) copied, 1.80012 s, 1.2 GB/s

```

# 包含缓存的实际速度
```bash
dd if=/dev/zero of=/root/kk/test_zero bs=2G count=1
0+1 records in
0+1 records out
2147479552 bytes (2.1 GB) copied, 1.80012 s, 1.2 GB/s
```

# 测试验证
因为看 ceph 文档的时候 当时也看到过 零星的一些知识 ，读写策略
所以就觉得 可能 虚机是可以 比单台物理机快的。 并且应该在理论上存在这样的 关系
power( 单副本虚机 ) < power( 单台物理机 ) <  power( 多副本虚机 )  < power( 所有物理机上限 )

# oflag 测试磁盘
## 在物理节点上
```bash

dd if=/dev/zero of=/root/kk/test_zero bs=2G count=1 oflag=direct wflag=direct
0+1 records in
0+1 records out
2147479552 bytes (2.1 GB) copied, 8.66371 s, 248 MB/s

# twice
2147479552 bytes (2.1 GB) copied, 8.95695 s, 240 MB/s

# third
2147479552 bytes (2.1 GB) copied, 8.56743 s, 251 MB/s


dd if=/dev/zero of=/root/kk/test_zero bs=8k count=262144 oflag=direct
262144+0 records in
262144+0 records out
2147483648 bytes (2.1 GB) copied, 26.069 s, 82.4 MB/s

# twice
2147483648 bytes (2.1 GB) copied, 23.399 s, 91.8 MB/s

# third
2147483648 bytes (2.1 GB) copied, 21.8463 s, 98.3 MB/s


```

## 在三副本虚机节点上

### normal 虚机
```bash

dd if=/dev/zero of=/root/kk/test_zero bs=2G count=1 oflag=direct
0+1 records in
0+1 records out
2147479552 bytes (2.1 GB) copied, 3.00279 s, 715 MB/s

dd if=/dev/zero of=/root/kk/test_zero bs=2G count=1 oflag=direct
2147479552 bytes (2.1 GB) copied, 2.73447 s, 785 MB/s


dd if=/dev/zero of=/root/kk/test_zero bs=2G count=1 oflag=direct
2147479552 bytes (2.1 GB) copied, 2.93444 s, 732 MB/s


dd if=/dev/zero of=/root/kk/test_zero bs=8k count=262144 oflag=direct
262144+0 records in
262144+0 records out
2147483648 bytes (2.1 GB) copied, 36.0823 s, 59.5 MB/s

# twice
2147483648 bytes (2.1 GB) copied, 37.0245 s, 58.0 MB/s

# third
2147483648 bytes (2.1 GB) copied, 37.4661 s, 57.3 MB/s

猜测 大概是 块太小的时候 调度成本太高

```

# 总结
在块 很大的时候， 三副本虚机 速度 超过了 单台物理机 2倍多，
在块 很小的时候，三副本虚机 甚至 不如单台物理机
所以对于高端需求，可以考虑尽可能增大 单词块大小。

因为 块 太小，网络通信 成本可能 太大，导致整体性能 骤减。



# 创建更多 副本 看看 增大 pool size 对性能的影响
```bash
osd pool create <pool> [<pg_num:int>] [<pgp_num:int>] [replicated|erasure] [<erasure_code_         create pool
 profile>] [<rule>] [<expected_num_objects:int>] [<size:int>] [<pg_num_min:int>] [on|off|warn]     
 [<target_size_bytes:int>] [<target_size_ratio:float>]  

osd pool create test_size12 size=12
```

# node01
```bash
ssh root@192.168.41.1 'mkdir -p /root/kk;dd if=/dev/zero of=/root/kk/test_zero bs=10M count=128 oflag=direct ;rm /root/kk/test_zero'
2048+0 records in
2048+0 records out
21474836480 bytes (21 GB) copied, 69.3638 s, 310 MB/s

```

# node02
```bash

ssh root@192.168.41.2 'mkdir -p /root/kk;dd if=/dev/zero of=/root/kk/test_zero bs=10M count=128 oflag=direct ;rm /root/kk/test_zero'
2048+0 records in
2048+0 records out
21474836480 bytes (21 GB) copied, 69.2626 s, 310 MB/s

```


# node03
```bash
ssh root@192.168.41.3 'mkdir -p /root/kk;dd if=/dev/zero of=/root/kk/test_zero bs=10M count=128 oflag=direct ;\rm /root/kk/test_zero'
2048+0 records in
2048+0 records out
21474836480 bytes (21 GB) copied, 69.2626 s, 310 MB/s

```

## size3 虚机
```bash
ssh root@192.168.141.12 'mkdir -p /root/kk;dd if=/dev/zero of=/root/kk/test_zero bs=10M count=128 oflag=direct ;\rm /root/kk/test_zero'
128+0 records in
128+0 records out
1342177280 bytes (1.3 GB) copied, 1.18654 s, 1.1 GB/s

```

## size10虚机
```bash

ssh root@192.168.141.16 'mkdir -p /root/kk;dd if=/dev/zero of=/root/kk/test_zero bs=10M count=128 oflag=direct ;\rm /root/kk/test_zero'
2048+0 records in
2048+0 records out
21474836480 bytes (21 GB) copied, 19.3196 s, 1.1 GB/s

```
## 单纯增加 pool size总结
```bash
上限 基本等于 物理机个数
但是 继续 联想，查看 crush 信息 ,发现 两个存储池 存储策略 都是 volume 但是 volume 的故障作用域 都是 host
ceph osd crush dump

ceph osd dump | grep volumes
-->
pool 1 'volumes' replicated size 2 min_size 1 crush_rule 0 

ceph osd dump | grep size10
-->
pool 4 'test_size10' replicated size 10 min_size 2 crush_rule 0 

# 查询 crush
ceph osd crush rule dump volumes
-->
{
    "rule_id": 0,
    "rule_name": "volumes",
    "ruleset": 0,
    "type": 1,
    "min_size": 1,
    "max_size": 10,
    "steps": [
        {
            "op": "take",
            "item": -1,
            "item_name": "volumes"
        },
        {
            "op": "chooseleaf_firstn",
            "num": 0,
            "type": "host"
        },
        {
            "op": "emit"
        }
    ]
}

```

# 尝试创建 osd 级别的 crush
```bash
ceph osd tree

ceph osd crush rule create-replicated test_osd10 volumes osd ssd

ceph osd crush rule dump test_osd10

# 应用 新的 osd 级别的 crush
ceph osd pool set test_size10 crush_rule test_osd10


ceph osd pool set test_size10 size 1


mkdir -p /root/kk;dd if=/dev/zero of=/root/kk/test_zero bs=10M count=128 oflag=direct ;\rm /root/kk/test_zero


```

